package grpc

import (
	"context"
	"fmt"
	"io"
	"log"
	"os"
	"path/filepath"

	"google.golang.org/protobuf/types/known/timestamppb"

	pb "github.com/aether/sync/api/proto"
	"github.com/aether/sync/internal/container"
	"github.com/aether/sync/internal/domain/entity"
)

// ChunkHandler ChunkService implementasyonu
type ChunkHandler struct {
	pb.UnimplementedChunkServiceServer
	container *container.Container
}

// NewChunkHandler yeni ChunkHandler oluÅŸturur
func NewChunkHandler(cont *container.Container) *ChunkHandler {
	return &ChunkHandler{container: cont}
}

// ChunkFile dosyayÄ± chunk'lar ve kaydeder
func (h *ChunkHandler) ChunkFile(ctx context.Context, req *pb.ChunkFileRequest) (*pb.ChunkFileResponse, error) {
	// Dosya kaydÄ±nÄ±n var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	file, err := h.container.FileRepository().GetByID(ctx, req.FileId)
	if err != nil || file == nil {
		// Dosya kaydÄ± yoksa oluÅŸtur
		fileInfo, err := os.Stat(req.FilePath)
		if err != nil {
			return &pb.ChunkFileResponse{
				Status: &pb.Status{
					Success: false,
					Message: fmt.Sprintf("Dosya bulunamadÄ±: %v", err),
					Code:    404,
				},
			}, nil
		}

		// Yeni dosya kaydÄ± oluÅŸtur
		folderID := req.FolderId
		if folderID == "" {
			folderID = "temp-folder-id"
		}
		newFile := entity.NewFile(folderID, filepath.Base(req.FilePath), fileInfo.Size(), fileInfo.ModTime())
		newFile.ID = req.FileId
		
		if err := h.container.FileRepository().Create(ctx, newFile); err != nil {
			return &pb.ChunkFileResponse{
				Status: &pb.Status{
					Success: false,
					Message: fmt.Sprintf("Dosya kaydÄ± oluÅŸturulamadÄ±: %v", err),
					Code:    500,
				},
			}, nil
		}
	}

	// Chunking use case'i Ã§aÄŸÄ±r
	chunks, globalHash, err := h.container.ChunkingUseCase().ChunkAndStoreFile(ctx, req.FileId, req.FilePath)
	if err != nil {
		return &pb.ChunkFileResponse{
			Status: &pb.Status{
				Success: false,
				Message: fmt.Sprintf("Dosya chunk'lanamadÄ±: %v", err),
				Code:    500,
			},
		}, nil
	}

	// Chunk'larÄ± proto'ya dÃ¶nÃ¼ÅŸtÃ¼r
	protoChunks := make([]*pb.ChunkInfo, len(chunks))
	var totalSize int64
	for i, chunk := range chunks {
		protoChunks[i] = convertChunkToProto(chunk)
		totalSize += chunk.Size
	}

	log.Printf("âœ… Dosya chunk'landÄ±: %s (%d chunks, global_hash: %s...)", req.FileId, len(chunks), globalHash[:16])

	return &pb.ChunkFileResponse{
		Status: &pb.Status{
			Success: true,
			Message: "Dosya baÅŸarÄ±yla chunk'landÄ±",
			Code:    200,
		},
		GlobalHash: globalHash,
		ChunkCount: int32(len(chunks)),
		TotalSize:  totalSize,
		Chunks:     protoChunks,
	}, nil
}

// GetFileChunks dosyanÄ±n chunk'larÄ±nÄ± getirir
func (h *ChunkHandler) GetFileChunks(ctx context.Context, req *pb.GetFileChunksRequest) (*pb.GetFileChunksResponse, error) {
	// Chunk'larÄ± yÃ¼kle
	chunks, err := h.container.ChunkingUseCase().LoadFileChunks(ctx, req.FileId)
	if err != nil {
		return &pb.GetFileChunksResponse{
			Status: &pb.Status{
				Success: false,
				Message: fmt.Sprintf("Chunk'lar yÃ¼klenemedi: %v", err),
				Code:    500,
			},
		}, nil
	}

	// File-chunk iliÅŸkilerini al
	fileChunks, err := h.container.ChunkRepository().GetFileChunks(ctx, req.FileId)
	if err != nil {
		return &pb.GetFileChunksResponse{
			Status: &pb.Status{
				Success: false,
				Message: fmt.Sprintf("File-chunk iliÅŸkileri alÄ±namadÄ±: %v", err),
				Code:    500,
			},
		}, nil
	}

	// Proto'ya dÃ¶nÃ¼ÅŸtÃ¼r
	protoChunks := make([]*pb.ChunkInfo, len(chunks))
	for i, chunk := range chunks {
		protoChunks[i] = convertChunkToProto(chunk)
	}

	protoFileChunks := make([]*pb.FileChunkInfo, len(fileChunks))
	for i, fc := range fileChunks {
		protoFileChunks[i] = &pb.FileChunkInfo{
			FileId:     fc.FileID,
			ChunkHash:  fc.ChunkHash,
			ChunkIndex: int32(fc.ChunkIndex),
		}
	}

	return &pb.GetFileChunksResponse{
		Status: &pb.Status{
			Success: true,
			Message: "Chunk'lar baÅŸarÄ±yla getirildi",
			Code:    200,
		},
		Chunks:     protoChunks,
		FileChunks: protoFileChunks,
	}, nil
}

// DownloadChunk chunk verisini stream olarak indirir
func (h *ChunkHandler) DownloadChunk(req *pb.DownloadChunkRequest, stream pb.ChunkService_DownloadChunkServer) error {
	ctx := stream.Context()

	// Chunk verisini oku
	chunkData, err := h.container.ChunkingUseCase().GetChunkData(ctx, req.ChunkHash)
	if err != nil {
		return fmt.Errorf("chunk verisi okunamadÄ±: %w", err)
	}

	// 64 KB paketler halinde stream et
	bufferSize := 64 * 1024
	totalSize := len(chunkData)
	offset := 0

	for offset < totalSize {
		end := offset + bufferSize
		if end > totalSize {
			end = totalSize
		}

		packet := &pb.ChunkDataResponse{
			Data:      chunkData[offset:end],
			Offset:    int32(offset),
			TotalSize: int32(totalSize),
		}

		if err := stream.Send(packet); err != nil {
			return fmt.Errorf("chunk paketi gÃ¶nderilemedi: %w", err)
		}

		offset = end
	}

	log.Printf("âœ“ Chunk indirildi: %s (%d bytes)", req.ChunkHash[:16]+"...", totalSize)
	return nil
}

// UploadChunk chunk verisini stream olarak yÃ¼kler
func (h *ChunkHandler) UploadChunk(stream pb.ChunkService_UploadChunkServer) error {
	ctx := stream.Context()

	var chunkHash string
	var totalSize int64
	var receivedData []byte

	// Stream'den paketleri al
	for {
		req, err := stream.Recv()
		if err == io.EOF {
			break
		}
		if err != nil {
			return fmt.Errorf("chunk paketi alÄ±namadÄ±: %w", err)
		}

		// Ä°lk paketten metadata al
		if chunkHash == "" {
			chunkHash = req.ChunkHash
			totalSize = req.TotalSize
			receivedData = make([]byte, 0, totalSize)
		}

		receivedData = append(receivedData, req.Data...)
	}

	// Chunk'Ä± kaydet
	chunk := entity.NewChunk(chunkHash, int64(len(receivedData)))
	
	// Chunk zaten var mÄ± kontrol et
	existingChunk, _ := h.container.ChunkRepository().GetByHash(ctx, chunkHash)
	wasDuplicate := existingChunk != nil

	if !wasDuplicate {
		// Repository'ye kaydet
		if err := h.container.ChunkRepository().Create(ctx, chunk); err != nil {
			return stream.SendAndClose(&pb.UploadChunkResponse{
				Status: &pb.Status{
					Success: false,
					Message: fmt.Sprintf("Chunk kaydedilemedi: %v", err),
					Code:    500,
				},
			})
		}
	}

	log.Printf("âœ“ Chunk yÃ¼klendi: %s (%d bytes, duplicate=%v)", chunkHash[:16]+"...", len(receivedData), wasDuplicate)

	return stream.SendAndClose(&pb.UploadChunkResponse{
		Status: &pb.Status{
			Success: true,
			Message: "Chunk baÅŸarÄ±yla yÃ¼klendi",
			Code:    200,
		},
		ChunkHash:    chunkHash,
		WasDuplicate: wasDuplicate,
	})
}

// VerifyFileIntegrity dosya bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ doÄŸrular
func (h *ChunkHandler) VerifyFileIntegrity(ctx context.Context, req *pb.VerifyFileIntegrityRequest) (*pb.VerifyFileIntegrityResponse, error) {
	// BÃ¼tÃ¼nlÃ¼k kontrolÃ¼ yap
	err := h.container.ChunkingUseCase().VerifyFileIntegrity(ctx, req.FileId, req.ExpectedGlobalHash)
	if err != nil {
		return &pb.VerifyFileIntegrityResponse{
			Status: &pb.Status{
				Success: false,
				Message: fmt.Sprintf("BÃ¼tÃ¼nlÃ¼k kontrolÃ¼ baÅŸarÄ±sÄ±z: %v", err),
				Code:    500,
			},
			IsValid: false,
		}, nil
	}

	return &pb.VerifyFileIntegrityResponse{
		Status: &pb.Status{
			Success: true,
			Message: "Dosya bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ doÄŸrulandÄ±",
			Code:    200,
		},
		IsValid:          true,
		ActualGlobalHash: req.ExpectedGlobalHash,
	}, nil
}

// GetDeduplicationStats deduplication istatistiklerini getirir
func (h *ChunkHandler) GetDeduplicationStats(ctx context.Context, req *pb.GetDeduplicationStatsRequest) (*pb.GetDeduplicationStatsResponse, error) {
	totalChunks, uniqueChunks, savings, err := h.container.ChunkingUseCase().GetDeduplicationStats(ctx)
	if err != nil {
		return &pb.GetDeduplicationStatsResponse{
			Status: &pb.Status{
				Success: false,
				Message: fmt.Sprintf("Ä°statistikler alÄ±namadÄ±: %v", err),
				Code:    500,
			},
		}, nil
	}

	// Deduplication ratio hesapla
	deduplicationRatio := float32(0)
	if totalChunks > 0 {
		deduplicationRatio = float32(uniqueChunks) / float32(totalChunks) * 100
	}

	// Disk kullanÄ±mÄ±nÄ± hesapla
	var diskUsage int64
	localChunks, _ := h.container.ChunkRepository().GetLocalChunks(ctx)
	for _, chunk := range localChunks {
		diskUsage += chunk.Size
	}

	return &pb.GetDeduplicationStatsResponse{
		Status: &pb.Status{
			Success: true,
			Message: "Ä°statistikler baÅŸarÄ±yla alÄ±ndÄ±",
			Code:    200,
		},
		TotalChunkReferences: totalChunks,
		UniqueChunks:         uniqueChunks,
		SavingsBytes:         savings,
		DeduplicationRatio:   deduplicationRatio,
		DiskUsageBytes:       diskUsage,
	}, nil
}

// CleanOrphanChunks orphan chunk'larÄ± temizler
func (h *ChunkHandler) CleanOrphanChunks(ctx context.Context, req *pb.CleanOrphanChunksRequest) (*pb.CleanOrphanChunksResponse, error) {
	// Ã–nce silinecek chunk'larÄ±n boyutunu al
	var freedBytes int64
	
	deletedCount, err := h.container.ChunkRepository().DeleteOrphanedChunks(ctx)
	if err != nil {
		return &pb.CleanOrphanChunksResponse{
			Status: &pb.Status{
				Success: false,
				Message: fmt.Sprintf("Orphan chunk'lar temizlenemedi: %v", err),
				Code:    500,
			},
		}, nil
	}

	freedBytes = int64(deletedCount) * 256 * 1024 // YaklaÅŸÄ±k

	log.Printf("ðŸ§¹ %d orphan chunk temizlendi (%d MB boÅŸaltÄ±ldÄ±)", deletedCount, freedBytes/(1024*1024))

	return &pb.CleanOrphanChunksResponse{
		Status: &pb.Status{
			Success: true,
			Message: fmt.Sprintf("%d orphan chunk temizlendi", deletedCount),
			Code:    200,
		},
		DeletedChunks: int32(deletedCount),
		FreedBytes:    freedBytes,
	}, nil
}

// Helper fonksiyonlar

func convertChunkToProto(c *entity.Chunk) *pb.ChunkInfo {
	if c == nil {
		return nil
	}

	return &pb.ChunkInfo{
		Hash:         c.Hash,
		Size:         c.Size,
		CreationTime: timestamppb.New(c.CreationTime),
		IsLocal:      c.IsLocal,
	}
}

